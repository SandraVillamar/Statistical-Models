---
title: "lab8"
output: html_document
---


```{r include=FALSE}
library(car)
library(alr4)
library(leaps)
library(MASS)
library(glmbb)
library(generalhoslem)
```

## Logistic Regression

We consider the HorseshoeCrab data: Here `y` is whether a female crab has a satellite (1=yes, 0=no).

```{r}
data(crabs)
crabs$color = relevel(crabs$color,ref = 'light')
m0=glm(y~1,family = binomial(link = logit),data=crabs)
m1=glm(y~color + width + weight + spine,family = binomial(link = logit),data=crabs)
summary(m1)
```
For `glm`, you can apply `anova()` to do analysis of deviance to test a submodel ($H_0$) against a larger model ($H_1$):

$H_0$ : y $\sim$ color + width + spine vs $H_1$ : y $\sim$ color + width + spine + weight

```{r}
m2 = glm(y~color + width + spine,family = binomial(link = logit),data=crabs)
anova(m2,m1)
```

Here the deviance is the likelihood ratio test statistic for testing $H_0$ vs $H_1$. Since it is chisquare distributed unde the null, we can apply the `pchisq()` function to compute the p-value. 

```{r}
1 - pchisq(1.4099, df = 1)

```

The p-value is 0.24, so we choose not to reject the null claiming there is no significant improvement in model performance after adding the predictor `weight`.






### Model selection

`step()` function also supports `glm()`

```{r}
step(m0,scope = formula(m1), direction="forward")
```

```{r}

step(m1, direction="backward")
```

### Model interpretation

```{r}
m3 = glm(y~color + width,family = binomial(link = logit),data=crabs)
m3$coefficients
round(exp(confint(m3)),3)
```
In logistic regression, the coefficients represent the change in the log odds of the response variable (i.e., the dependent variable) for a one-unit increase in the corresponding predictor variable (i.e., the independent variable), holding all other predictor variables constant.

In this case, for one unit increase in width, the odds increases by a factor of `exp(0.46795598) = 1.597`. This is about 60\% increase in odds of `y`. 

### Goodness of fit

The Hosmer-Lemeshow test is a statistical test used to assess the goodness-of-fit of a logistic regression model.

```{r}
logitgof(crabs$y,m3$fitted.values,g=10)
```

The p-value for Hosmer and Lemeshow test is greater than 0.05 which indicates the model fits the data well.

### visualize the predicted probability

`predict()`  function also supports `glm()`

```{r}
plot(crabs$width,crabs$y,xlab = 'width',ylab = 'y')
curve(expr = predict(object = m3 , newdata = data.frame(width = x,color = rep('light',length(x))), type = "response") , col = "yellow", add= TRUE ,lwd=3)
curve(expr = predict(object = m3 , newdata = data.frame(width = x,color = rep('medium',length(x))), type = "response") , col = "darkred", add= TRUE ,lwd=3)
curve(expr = predict(object = m3 , newdata = data.frame(width = x,color = rep('dark',length(x))), type = "response") , col = "darkgreen", add= TRUE ,lwd=3)
curve(expr = predict(object = m3 , newdata = data.frame(width = x,color = rep('darker',length(x))), type = "response") , col = "darkblue", add= TRUE ,lwd=3)

legend(30,0.4,legend = c('color=light','color=medium','color=dark','darker'),col = c("yellow","darkred","darkgreen","darkblue"),lty = c(1,1,1,1),bty = 'n')
```


## Poisson regression

```{r}
mp = glm(satell ~ color + width, family = poisson(link = log), data = crabs)
summary(mp)
```

Regression coefficients
```{r}
mp$coefficients 
round(exp(confint(mp)),3)

```
In Poisson regression, the coefficients represent the change in the log of the expected count of the response variable (i.e., the dependent variable) for a one-unit increase in the corresponding predictor variable (i.e., the independent variable), holding all other predictor variables constant.

In this case, for one unit increase in width, the expected count of the satellites increase by `exp(0.1493427) = 1.161`. This is about 16\% increase. 

